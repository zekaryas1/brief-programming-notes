---
title: 06.1 More on Consistency
date created: Monday, August 7th 2023, 11:38:12 am
date modified: Sunday, March 17th 2024, 4:08:09 pm
---

# 06.1 More on Consistency

## Read-me First

- This website has a simpler, well format beginner level explanation about consistency
	- [Consistency Patterns - System Design](https://systemdesign.one/consistency-patterns/)
- when we have data that is replicated at multiple nodes, it must be synchronized across replicas.
- this means, since we have replicated data, the operations we do on one server must be replicated to the other replica servers too.
	- one reason for this importance could be servers could be using a cache which will be affected by every read/write operation.
	- and operation happening on server A, must also happen to all it's replicas

## Consistency Models

- what is consistency models?
	- the degree of data consistency
- ![](https://book.mixu.net/distsys/images/google-transact09.png)
- Image explanation
	- The diagram below, adapted from Ryan Barret at [Google](https://www.google.com/events/io/2009/sessions/TransactionsAcrossDatacenters.html), describes some of the aspects of the different options
	- M/S stands for Master/Slave replication
	- 2PC stands for Two phase commit
- possible types of consistency models
	1. strong consistency
		- read operations return the most up-to-date information
		- clients never sees out-of-date data.
		- blocks new operations(both read and writes), until every replica agreed on the current write.
		- algorithms
			- synchronous Master/slave replication
			- quorum based: Paxos/raft
				- most common, will be discussed next
			- 2PC
				- mostly used in relational db design, not used in modern distributed systems
	2. weak consistency
		- read operations may not see the most updated value
	3. eventual consistency
		- specific form of weak consistency
		- given enough time -> all updates are propagated -> resulting replicas to be consistent
		- algorithms
			- asynchronous master/slave replication
			- gossip protocol
		- systems that use eventual consistency
			- *dynamoDB, Cassandra*

## Quorum Consensus

- Quorum consensus is a type of consensus algorithm that relies on a majority or a threshold of nodes in a distributed system to reach an agreement or make decisions.
- In quorum-based algorithms,
	- a quorum refers to a subset of nodes that need to participate and agree on a decision for it to be considered valid.
	- usually the quorum is N = (n(total servers)/2 + 1)
- *quorum consensus* can guarantee consistency for both read and write operations.

```
in quorum consensus
N = number of replicas
W = for a write operation to be considered succesful, the operation must be acknowleged from W replicas
R = for a read operation to be considered succesful, the operation must be acknowledged from R replicas
```

- for instance, if we have, N=3, W=1
	- a coordinator will try to run the write operation to every N replicas, but will only wait for 1 server acknowledgment before it declares the write operation successful.
- *these variables must be configured to fit the business requirements*
- here are the possible N,W,R setups?

```
R=1, W=N -> for system optimized for a fast read
W=1, R=N -> for system optimized for a fast write
W+R > N  -> for system optimized for strong consistency, usually W=R=(N-1)
W+R <= N  -> for systems with weak consistency
```

- example of quorum based algorithms are
	- paxos
	- raft: designed as an alternative to the Paxos

## Which Consistency Model to Use

- ![cap theorem image](https://book.mixu.net/distsys/images/CAP.png)
- strong consistency is not ideal for high available system, it'll be too slow, so we will need to use *eventual consistency*  
- How eventual consistency works?
	- eventual consistency allows inconsistent values to enter the system and force the client to read the values to reconcile.
	- but how does reconciliation works?
		- the next section explains how reconciliation works with versioning

## Conflict/Inconsistency Resolution with Versioning

- replication
	- replication gives high availability but causes inconsistencies among replicas.
		- *how do we solve to conflicting data*
- there are different kind of versioning?
	1. using timestamp
		- each data has a timestamp as a version number
		- new timestamp will be accepted over old timestamp
		- caveat: time must be synchronized or guaranteed to be the same on all machines
	3. `Lamport` clock
		- every operation has a counter
			- read/write increase the counter by 1
		- operation with more counter will be accepted over operation over less counter
	4. vector clock
		- extension of `Lamport` clock
		- incorporated both the version counter and the new data.
- vector clock
	- are used to solve inconsistency problems.
	- what is versioning?
		- means treating each data modification as a new immutable version of data.
	- what is vector clock?
		- A vector clock is a [server, version] pair associated with a data item.
		- It can be used to check if one version precedes, succeeds, or in conflict with others.
- more on vector clock?
	- Assume a vector clock is represented by D([S1, v1], [S2,v2],…,[Sn, vn]), where D is a data item, v1 is a version counter, and s1 is a server number
	- if data item D is written to server si, the system must perform one of the following tasks.
		- increment vi if [si, vi] exists
		- otherwise, create a new entry [si, 1]
- pros and cons of vector clocks?
	- pros
		- can resolve conflicts
	- cons
		- adds complexity to the client because it needs to implement conflict resolution logic
		- the [server, version] pairs in the vector clock could grow rapidly
			- *to fix size we set a threshold(max size), and if it exceeds the limit, the oldest pairs are removed*

## How to Detect Failures

- one inefficient way to detect server status is *all-to-all multicasting*
	- where every node broadcasts information to all other nodes.
	- pros:
		- straightforward to implement
	- cons:
		- with several nodes e.x 1M, it will be intensive to work with multicasting.
- *Gossip protocol*, how it works?
	- each node has a table consisting of node's id, and heartbeat counter with last updated time.
	- each node periodically increments its heartbeat counter
	- each node periodically sends heartbeats to a set of random nodes, which in turn propagate to another set of nodes
	- once node receive a heartbeats, table is updated to the latest info.
	- if the a nodes's heartbeat has not increased for more than predefined periods, the member is considered as offline.

```python
offline = []

for m in members:
	if (curr_time - last_updated_time[m]) > N:
		offline.append(m)
```

# Readme

- [Distributed systems for fun and profit](https://book.mixu.net/distsys/single-page.html)
- [Lamport Clocks | Kevin Sookocheff](https://sookocheff.com/post/time/lamport-clock/)
- [Vector Clocks Explained](https://riak.com/why-vector-clocks-are-easy/)
	- [Why Vector Clocks Are Hard – Riak](https://riak.com/why-vector-clocks-are-hard/)
